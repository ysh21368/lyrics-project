# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UbLZUDYRavxczEOPtOBp9VWEBy3cYsu8
"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/drive/MyDrive/final_pre.xls')

data.head()

!pip install transformers -U
!pip install -e '.[dev]'

##
## 대표 곡 index num from final_pre.xls
## 0 : '가로수 그늘아래 서면' / 39 : '사랑 먼지' / 58: '사랑은...향기를 남기고' / 77 : '1,2,3,4 (원,투,쓰리,포)' / 
## 97 : 임과 함께 / 114 : '취향저격' / 134 : 'Touch My Body'
num_list = [0,39,58,77,97,114,134]

## 대표 7곡에 점수를 매긴 곡들 정보
dict_97 = {'제비처럼':4.8,'사랑 그 아름답고 소중한 얘기들 (CF - 포스코 For happiness 편)': 4.8,'일편단심 민들레야': 4.5,'흙에 살리라':4.9,'내가 만일':4.9,'바람의 노래':4.5,'새들처럼':4.5,'하늘바라기 (Feat. 하림)':4.8,'그리운 사람은':4.5,'아름다워라 그대':4.5}
dict_0 = {'그날':4.8,'하늘바라기 (Feat. 하림)':4.8,'내 님의 사랑은':4.6,'아름다워라 그대':4.5,'화요일에 비가 내리면':4.5,'벚꽃 엔딩':4.5,'세월 가면':4.7,'연인이여':4.7,'또 다른 만남을 위해':4.5,'저 별과 달을':4.7,'방황':4.5}
dict_39 = {"사랑일기": 4.8,'바보처럼':4.8,'Cry':4.5,'난(亂)':4.7,'잘가요 내사랑':4.5,'사랑의 숲에서 길을 잃다 (Feat. 임창정)':4.9,'전부 너였다':4.5,'눈물이 나요':4.5,'꿈의 대화':4.5,'I Believe':4.8,'혼잣말':4.5,'불꽃 (Feat. 개리)':4.7}
dict_58 = {"사랑에 미치면 (Feat. 아웃캐스트 빅보이 & J.Y.Park 'The Asiansoul')":4.8, '눈물샘':4.7,'Blue Moon':4.7,'사랑이 술을 가르쳐 (Feat. 백찬 From 8eight)':4.7,'1st':4.6,'붉은 노을':4.5,'어떻게 지내 (Prod. By VAN.C)':4.5,'사랑해요 우리 (Feat. 거미)':4.5,"BLUE":4.5,'Sea Of Love':4.5,'Officially Missing You':4.6}
dict_77 =  {"I Don't Care":4.9,"DON'T TOUCH ME":4.5,'U & I': 4.6,'Be OK (Feat. 배치기)': 4.6,'어때 (Feat. 하림)':4.5,'Go Away':4.9,"I'm Sorry":4.5}
dict_114 = {"음오아예 (Um Oh Ah Yeh)":4.9,"넌 너무 야해 (Feat. 긱스) (The Way You Make Me Melt)":4.6, '갖고놀래 (Feat. 다이나믹 듀오)':4.6,'사랑스러워':4.8, 'All I Wanna Do (K) (Feat. Hoody, Loco)':4.5,'10점 만점에 10점':4.9,'My Style (Hidden Track)':4.5,'심쿵해 (Heart Attack)':4.5}
dict_134 = {'Diva':4.9,'작은 것들을 위한 시 (Boy With Luv) (Feat. Halsey)':4.8,'Falling In Love':4.7,'Fly Boy':4.5,'CALL ME BABY':4.5,'Kiss (Feat. CL)':4.6,'유혹의 소나타':4.7,'花요일 (Blooming Day)':4.5,'FANCY':4.5,'내 남자 친구에게':4.7}

tot_song_title = data['비교노래제목'].to_list()

## 점수를 매긴 대상 이외에는 모두 0점을 준다. --> 평균 10곡 정도에 점수 차등 점수 매칭
dict_list = [dict_97, dict_0,  dict_39, dict_58, dict_77, dict_114, dict_134]
for dict_ in dict_list:
    for name in tot_song_title:
        if name not in dict_.keys():
            dict_[name] = 0.1

!pip install datasets
!pip install -U sentence-transformers
##
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, util
from sentence_transformers import losses
import warnings
warnings.filterwarnings(action='ignore')


## klue data set 
# klue_sts_train = load_dataset("klue", "sts", split='train[:90%]')
# klue_sts_valid = load_dataset("klue", "sts", split='train[-10%:]') # train의 10%를 validation set으로 사용
# klue_sts_test = load_dataset("klue", "sts", split='validation')
klue_set = load_dataset("klue", "sts")

##

##
from datasets import Dataset
from sentence_transformers.readers import InputExample

df = pd.DataFrame(columns=["sentence1", "sentence2", "labels.label"])

num_list = [0, 39, 58, 77, 97, 114, 134]
dict_list = [dict_0, dict_39, dict_58, dict_77, dict_97, dict_114, dict_134]

for dict_ ,i in  zip(dict_list,num_list):       
    for title, label in dict_.items():

        # Get the sentence1 value from the first row of data
        sentence1 = data.loc[i, '비교노래가사']

        # Get the sentence2 value corresponding to the current title from data
        sentence2 = data[data['비교노래제목'] == title]['비교노래가사'].iloc[0]

        #label = label /5
        #Append a new row to the DataFrame with the values for sentence1, sentence2, and label
        row = {"sentence1": sentence1, "sentence2": sentence2, "labels.label":   label}
        df = df.append(row, ignore_index=True)


## golden dataset 생성 
add_dataset  = Dataset.from_pandas(df)

## augument dataset 생성
augu = pd.read_csv('augument.csv')
augument_dataset = Dataset.from_pandas(augu)


train_samples = []
dev_samples = []
test_samples = []

# KLUE STS 내 훈련, 검증 데이터 예제 변환
for phase in ["train", "validation"]:
    examples = klue_set[phase]

    for example in examples:
        score = float(example["labels"]["label"]) / 5.0  # 0.0 ~ 1.0 스케일로 유사도 정규화

        inp_example = InputExample(
            texts=[example["sentence1"], example["sentence2"]], 
            label=score,
        )

        if phase == "validation":
            dev_samples.append(inp_example)
        else:
            train_samples.append(inp_example)


## golden set & augument set merge to train_samples
def make_examples (dataset) :
    for example in  dataset:
        score = float(example["labels.label"]) / 5.0  # 0.0 ~ 1.0 스케일로 유사도 정규화

        inp_example = InputExample(
            texts=[example["sentence1"], example["sentence2"]], 
            label=score,
        )
        train_samples.append(inp_example)
    return train_samples

make_examples(augument_dataset)
make_examples(add_dataset)

##
from torch.utils.data import DataLoader
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Train Dataloader
train_dataloader = DataLoader(
    train_samples,
    shuffle=True,
    batch_size=32, # 32 (논문에서는 16)
)

# Evaluator by sts-validation
dev_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
    dev_samples,
    name="sts-dev",
)

# Evaluator by sts-test
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
    test_samples,
    name="sts-test",
)
##
from sentence_transformers import SentenceTransformer, models

# Load Embedding Model
##'snunlp/KR-SBERT-V40K-klueNLI-augSTS'  // 'klue/roberta-base'
sunlp = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'
robert = 'klue/roberta-base'
embedding_model = models.Transformer(
    model_name_or_path=robert , 
    max_seq_length=256,
    do_lower_case=True
)

# Only use Mean Pooling -> Pooling all token embedding vectors of sentence.
pooling_model = models.Pooling(
    embedding_model.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True,
    pooling_mode_cls_token=False,
    pooling_mode_max_tokens=False,
)
model = SentenceTransformer(modules=[embedding_model, pooling_model])

##
from sentence_transformers import losses
import math
# config
sts_num_epochs = 2
train_batch_size = 2
sts_model_save_path = "model"

# Use CosineSimilarityLoss
train_loss = losses.CosineSimilarityLoss(model=model)
# linear learning-rate warmup steps
warmup_steps = math.ceil(len(train_samples) * sts_num_epochs / train_batch_size * 0.1) #10% of train data for warm-up
# Training
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    evaluator=dev_evaluator,
    epochs=sts_num_epochs,
    evaluation_steps=int(len(train_dataloader)*0.1),
    warmup_steps=warmup_steps,
    output_path=sts_model_save_path,
    save_best_model=True
)

